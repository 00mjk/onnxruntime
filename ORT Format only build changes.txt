ORT Format only build changes

High level approach:
  - Serialize Graph instance including Node instances and initializers
    - subgraphs are serialized the same way
    - current assumption is that EPs to be used in deployed version should be known, and L1 and L2 transformers can be run ahead of time 
      - we can add in transformers if this isn't the case though, but cost will vary depending on what's required
    - needed to add storage of SinceVersion in Node so ONNX OpSchema isn't required 
  - Serialize SessionState
    - currently this is just the KernelCreateInfo
    - allows kernel to be instantiated without dependency on ONNX for schema lookup/validation
      - e.g. if kernel is templatized on type this will save the KCI for the correct version
    - refactored to remove usage of SearchKernelRegistry multiple times for a Node 
      - single lookup in finalization of SessionState to save KCI
        - allocation planner and kernel registry manager now use saved KCI
    - somewhat significant refactoring so that finalization is inside SessionState and InferenceSession usage is now cleaner
      - recent cleanup of SessionState made this viable
  - Currently serializing a few things using a protobuf blob
    - done for expediency as we currently have a hard dependency on the protobuf types

Initial status from RelWithDebInfo build
  - need to build that way to get per-symbol info on Windows. MinSizeRel is signficantly smaller

=====
TODO: Need better baseline with more equivalent builds that both exclude RTTI and ML ops
=====

ONNX: 
  26KB (-800KB)
  - Library just includes data type helpers

Optimizers: 
  45KB (-277KB)
  Kept ones required post-transform (required if we enable NCHW)
    - graph tranformer, insert cast, memcpy
    - Excluded all others
    - could remove all if no transformations after load

Framework:
  346KB (-50KB)
  - Removed Custom op support
    - currently no good separation between just getting the kernel defs and the schemas
  - Removed implementation of GraphPartitioner
    - serialized model must be partitioned first
  - Remove VerifyKernelDef
  Can save in a few places:
    - 49KB in data_types and a lot of that is for ML types
    - 31KB for allocation planner: could serialize
    - 16KB parallel executor: could exclude
    - 7KB BFCArena::DumpMemoryLog + DebugString could be excluded
    - 7KB dealing with TensorProto to OrtValue - could just create OrtValue from offset in flatbuffer/flexbuffer     
      - probably a few other places where doing so would save multiple KB


Graph:
  88KB (-210KB)
  - Cut out initial set of obvious methods that weren't required. 
    - e.g. ctors used for load from proto (Model and Graph), ToProto, Serialization, Node::Op (returns onnx::OpSchema instance), OnnxRuntimeOpSchemaRegistry
  - Pieces required for transformers are still supported but could be removed
    - Could save about 20KB from that
      - e.g. Resolve, AddNode, AddEdge, RemoveEdge, etc. 
  - No support for Function 
    - tightly bound to OpSchema class in ONNX 
      - can't bring in in schema.cc from ONNX without splitting that up more
      - TBD if Function support is required
  - Can remove some training related code for some minor savings
    - consumer/provider info
  - Can remove graph_utils if no transformer support
    - only 5.6KB though

Providers:
  2.1MB (-427KB)
  *** this is slightly off as Einsum shows up as significant diff. need equivalent base commit id ***
  - Excluded ML ops which was most of diff
  - Currently includes all contrib ops
    - can split out based on category
      - Experimental/NCHWc/Optimizer related (inc. BERT specific)/
  

Session:
  280KB (-40KB)
  - Excluded custom ops and ctors involving loading ModelProto
  - Can remove support for config in model and exclude JSON parser 
    - Saving: 65KB (remove inference_session_utils.*)
  - C API could be minimized
    Currently 100KB
      - probably a lot we can remove to minimized
      - lot of cost to support ML types such as Map that could be removed based on --disable_ml_ops

MLAS:
  260KB
  - Not sure what ARM size will be
  - AVX512 support is expensive. could make that optional
    - Saving ~110KB

Common:
  79KB
  - telemetry is 6.6KB + some cost in InferenceSession
  - profiler is 7.5KB + some cost in InferenceSession
    - could be made optional

Other:
  - Exclude RE2
    - 163KB

-----

Other notes:
  - Still have a dependency on ONNX PB
    - More work to replace though given internal use of *Proto types in many places
    - Current size is 83KB
      - Could probably cut down by excluding training and function related pieces


  - Are we paying cost for templatized containers in multiple libs
    - e.g. Graph has Hash of unique_ptr<NodeArg> for 552 bytes
    - may not exist in MinSizeRel build

===============================
Testing setup:

Commit id from master (2020-07-08, should be ~ORT 1.4 release): 6d6b6b54a544f69fb201a9aab7097358f6a440ff
Updated: ort.vs19.perf.master, ort.serialize and ort.deserialize are all on the same base commit id

NOTE: Sizes for specific savings are based on a RelWithDebInfo build as we need the pdb for SizeBench to work.
      These need to be roughly adjusted by the difference between that and a MinSizeRel build (~20% smaller so reduce saving by that).
      Summary includes total size from MinSizeRel build. 

Build flags:
  Master: --disable_rtti --disable_ml_ops
  Deserialize: Add --ort_model_format 

MinSizeRel build
  Base: .\build.bat --parallel --build_shared_lib --build_wheel --cmake_generator="Visual Studio 16 2019" --config=MinSizeRel --disable_rtti --disable_ml_ops --build --update --skip_submodule_sync
  Deserialize: .\build.bat --parallel --build_shared_lib --build_wheel --cmake_generator="Visual Studio 16 2019" --config=MinSizeRel --ort_model_format --disable_rtti --disable_ml_ops --build --update --skip_submodule_sync

Master
    RelWithDebInfo  onnxruntime.dll	    5,236,736
    MinSizeRel      onnxruntime.dll	    4,398,080

Ort.deserialize (RelWithDebInfo.1 and MinSizeRel.1 folders)
    RelWithDebInfo  onnxruntime.dll	    3,833,344
    MinSizeRel      onnxruntime.dll	    3,231,744
                    onnxruntime.zip	    1,223,231
=================================

Next step:

  Reduced onnx_proto
    68KB (-15KB)
    - no usage of onnx-operators-ml.proto
    - comment out training related types
    - fix some places to not directly include schema.h from onnx as that's broken with these changes 

  Removed re2 dependency and Tokenizer contrib op
    -163KB

  Removed parsing of json from Model to save json parser cost
    -70KB (68 in inference_session_utils.obj and a couple in inference_session.obj) 

  Ort.deserialize (RelWithDebInfo.2 and MinSizeRel.2 folders)
      RelWithDebInfo  onnxruntime.dll	    3,553,280
      MinSizeRel      onnxruntime.dll	    2,976,256
                      onnxruntime.zip	    1,106,631

=================================
Next steps (roughly ordered on cost/benefit):

1.
  - remove optimzer support
    - -5KB for InferenceSession::TranformGraph
    - can disable a lot more code in Graph as well
    - can try building with no contrib ops to see what size is possible
      - however the disable unused ops approach is what would be needed for production given we want to save an optimized model which will require some contrib ops in most cases
    - can fully remove onnxruntime_optimizer (-46.7KB)
      - memcpy and cast transformers
      - can be removed if we're not going to change the nodes in the graph
    - can remove graph_utils (-5.7KB)
    - guesstimate ~80KB saving

2.
  - excise ML types better
    - remove from data type utils etc.
    - remove from C API
      - C API is currently 100KB which is way too large 
    - guesstimate ~80KB saving

3.
  - exclude AVS512 from MLAS 
    - expect -110KB

4.
  - build with no contrib ops
    - expect -700KB
    - not totally viable as per above notes - need contrib ops for optimized models but incremental growth to add those

5.
  - serialize allocation plan
    - allocation planner is 45KB

5.
  - make telemetry optional
    - currently only user of ModelProto metadata as well
    - and a few KB saving

6.
  - reduce onnx_proto further
    - could try and remove TensorProto dependency
      - could cut out ORT code dealing with TensorProto for a good reduction
        - heavy change to implement though
      - doing so implies no optimizers as the implementations are currently heavily dependent on working at the TensorProto level

7.
  - Completely remove protobuf dependency (-100KB)
    - Currently (v2) 68KB for onnx_proto and 36KB for libprotobuf-lite

8.
  - Small cleanups
    - More parts of SessionState could be excluded (maybe ~10KB saving)
    - If we didn't have a dependency on TensorProto about 8KB from session_state_utils can go
    - parallel executor (-17KB)

Other:
  - pull in changes from android testing to disable exceptions
    - very large changeset though 
    - was ~430KB but will be smaller with more things excluded like ONNX type/shape inferencing
      - guesstimate of -250KB

  - experiment with limited types
    - can get ballpark figure for a couple of models
    - assuming this will be on top of per-model/s op reductions
      - possible we could define a subset of types though to have a smaller binary with no per-model stage of a build
    - e.g. Cast (79KB), OneHot (35.5KB) and the reduction ops (140.8KB) could be much smaller  with fewer types
      - Cast can be reworked either way be closer to half the current size

  - use printf formatting for all log messages
    - TF Lite takes this approach to avoid cost of convenient iostream formatting
    - onnxruntime::MakeString is a significant cost in various places
      - use by ORT_MAKE_STATUS and ORT_THROW 

  - Framework is 342KB
    Should be 100KB saving achievable from small changes
      25: remove ML support from data types
      32: serialize allocation plan
      7: remove debug logging from BFCArena
      17: remove tensorprotoutils if we just create OrtValue instances from serialized data 
      17: parallel executor
      7: session_state_utils support for copying TensorProto to OrtValue
    ? 8: tensor_allocator (was added for Training, not sure if needed/used in inferencing)

Getting into ballpark of 750KB infrastructure + ops
  - assuming protobuf depenency can be removed
  