ORT Format only build changes

High level approach:
  - Serialize Graph instance including Node instances and initializers
    - subgraphs are serialized the same way
    - current assumption is that EPs to be used in deployed version should be known, and L1 and L2 transformers can be run ahead of time 
      - we can add in transformers if this isn't the case though, but cost will vary depending on what's required
    - needed to add storage of SinceVersion in Node so ONNX OpSchema isn't required 
  - Serialize SessionState
    - currently this is just the KernelCreateInfo
    - allows kernel to be instantiated without dependency on ONNX for schema lookup/validation
      - e.g. if kernel is templatized on type this will save the KCI for the correct version
    - refactored to remove usage of SearchKernelRegistry multiple times for a Node 
      - single lookup in finalization of SessionState to save KCI
        - allocation planner and kernel registry manager now use saved KCI
    - somewhat significant refactoring so that finalization is inside SessionState and InferenceSession usage is now cleaner
      - recent cleanup of SessionState made this viable
  - Currently serializing a few things using a protobuf blob
    - done for expediency as we currently have a hard dependency on the protobuf types

Initial status from RelWithDebInfo build
  - need to build that way to get per-symbol info on Windows. MinSizeRel is signficantly smaller

ONNX: 
  26KB (-800KB)
  - Library just includes data type helpers

Optimizers: 
  45KB (-277KB)
  Kept ones required post-transform (required if we enable NCHW)
    - graph tranformer, insert cast, memcpy
    - Excluded all others
    - could remove all if no transformations after load

Framework:
  346KB (-50KB)
  - Removed Custom op support
    - currently no good separation between just getting the kernel defs and the schemas
  - Removed implementation of GraphPartitioner
    - serialized model must be partitioned first
  - Remove VerifyKernelDef
  Can save in a few places:
    - 49KB in data_types and a lot of that is for ML types
    - 31KB for allocation planner: could serialize
    - 16KB parallel executor: could exclude
    - 7KB BFCArena::DumpMemoryLog + DebugString could be excluded
    - 7KB dealing with TensorProto to OrtValue - could just create OrtValue from offset in flatbuffer/flexbuffer     
      - probably a few other places where doing so would save multiple KB


Graph:
  88KB (-210KB)
  - Cut out initial set of obvious methods that weren't required. 
    - e.g. ctors used for load from proto (Model and Graph), ToProto, Serialization, Node::Op (returns onnx::OpSchema instance), OnnxRuntimeOpSchemaRegistry
  - Pieces required for transformers are still supported but could be removed
    - Could save about 20KB from that
      - e.g. Resolve, AddNode, AddEdge, RemoveEdge, etc. 
  - No support for Function 
    - tightly bound to OpSchema class in ONNX 
      - can't bring in in schema.cc from ONNX without splitting that up more
      - TBD if Function support is required
  - Can remove some training related code for some minor savings
    - consumer/provider info
  - Can remove graph_utils if no transformer support
    - only 5.6KB though

Providers:
  2.1MB (-427KB)
  - Excluded ML ops which was most of diff
  - Currently includes all contrib ops
    - can split out based on category
      - Experimental/NCHWc/Optimizer related (inc. BERT specific)/
  

Session:
  280KB (-40KB)
  - Excluded custom ops and ctors involving loading ModelProto
  - Can remove support for config in model and exclude JSON parser 
    - Saving: 65KB (remove inference_session_utils.*)
  - C API could be minimized
    Currently 100KB
      - probably a lot we can remove to minimized
      - lot of cost to support ML types such as Map that could be removed based on --disable_ml_ops

MLAS:
  260KB
  - Not sure what ARM size will be
  - AVX512 support is expensive. could make that optional
    - Saving ~110KB

Common:
  79KB
  - telemetry is 6.6KB + some cost in InferenceSession
  - profiler is 7.5KB + some cost in InferenceSession
    - could be made optional

-----
Other notes:
  - Still have a dependency on ONNX PB
    - More work to replace though given internal use of *Proto types in many places
    - Current size is 83KB
      - Could probably cut down by excluding training and function related pieces

===============================
Testing setup:

Commit id from master (2020-07-08, should be ~ORT 1.4 release): 6d6b6b54a544f69fb201a9aab7097358f6a440ff
Updated: ort.vs19.perf.master, ort.serialize and ort.deserialize are all on the same base commit id

NOTE: Sizes for specific savings are based on a RelWithDebInfo build as we need the pdb for SizeBench to work.
      These need to be roughly adjusted by the difference between that and a MinSizeRel build (~20% smaller so reduce saving by that).
      Summary includes total size from MinSizeRel build. 

Build flags:
  Master: --disable_rtti --disable_ml_ops
  Deserialize: Add --ort_model_format 

MinSizeRel build
  Base: .\build.bat --parallel --build_shared_lib --build_wheel --cmake_generator="Visual Studio 16 2019" --config=MinSizeRel --disable_rtti --disable_ml_ops --build --update --skip_submodule_sync
  Deserialize: .\build.bat --parallel --build_shared_lib --build_wheel --cmake_generator="Visual Studio 16 2019" --config=MinSizeRel --ort_model_format --disable_rtti --disable_ml_ops --build --update --skip_submodule_sync

Master
    RelWithDebInfo  onnxruntime.dll	    5,236,736
    MinSizeRel      onnxruntime.dll	    4,398,080

Ort.deserialize (RelWithDebInfo.1 and MinSizeRel.1 folders)
    RelWithDebInfo  onnxruntime.dll	    3,833,344
    MinSizeRel      onnxruntime.dll	    3,231,744
                    onnxruntime.zip	    1,223,231

Commits: 2020-07-08 skottmckay/GraphExperiment                  
=================================

Next step:

  Reduced onnx_proto
    68KB (-15KB)
    - no usage of onnx-operators-ml.proto
    - comment out training related types
    - fix some places to not directly include schema.h from onnx as that's broken with these changes 
    - NOTE: This removed OpSchema so wouldn't be able to run an optimizer. 
      Would need to replace the protobuf based OpSchema with a local struct

  Removed re2 dependency and Tokenizer contrib op
    -163KB

  Removed parsing of json from Model to save json parser cost
    -70KB (68 in inference_session_utils.obj and a couple in inference_session.obj) 

  Ort.deserialize (RelWithDebInfo.2 and MinSizeRel.2 folders)
      RelWithDebInfo  onnxruntime.dll	    3,553,280
      MinSizeRel      onnxruntime.dll	    2,976,256
                      onnxruntime.zip	    1,106,631

Commits: 2020-07-09 skottmckay/GraphExperiment                  

=================================

Next step:

  Removed optimizer support 
    - removed usage of all things in optimizer library
      - testing showed it wasn't possible to just have L3 support without significant changes so fully excluding makes more sense
    - removed Graph::Resolve related code 
  
  Remove ML types from data types and C API

  Remove a few things from SessionState that we weren't using in this build type
    - e.g. KCI population code and serialization code

  Remove training related code to reduce execution to just nodes producing outputs

  Savings:
    session:   50.6KB 
    optimizer: 46.7KB
    graph:     39.5KB
    framework: 16.6KB

    Breakdown
      session: 
        -37.6 C API from reducing types
        -13.9 inference_session from removing tranformer related code
      graph:
        -34 from removing Graph::Resolve related pieces (BuildConnections, PerformTopologicalSortAndCheckIsAcyclic, CleanUnusedInitializers) and graph editing (AddNode, Node::Init)
        -5.7 from excluding graph_utils (moved IsConstantInitializer into Graph class)
      framework:
        -12.9 data_types 
         -5.8 session_state mainly from removing 'ToBeExecutedNodes' related pieces, PopulateKernelCreateInfo

  Ort.deserialize (RelWithDebInfo.3 and MinSizeRel.3 folders)
      RelWithDebInfo  onnxruntime.dll	    3,394,048  (-155.5KB)
      MinSizeRel      onnxruntime.dll	    2,845,696  (-127.5KB)
                      onnxruntime.zip	    1,055,203

=================================

Next step:

  Build with no contrib ops just to get a data point
    - NOTE: This is too extreme for an optimized model as we need things like FusedConv

  Ort.deserialize (RelWithDebInfo.4 and MinSizeRel.4 folders)
      RelWithDebInfo  onnxruntime.dll	    3,038,720  (-347.0KB)
      MinSizeRel      onnxruntime.dll	    2,561,536  (-277.5KB)
                      onnxruntime.zip	    933,805

=================================

Next step:

  Re-enable contrib ops 
  Setup build with just the ops used
    - added support for using multiple models as input
    - setup the various op registries to handle commenting out all ops
    - found some issues with op registrations (Cast, Less, Greater)
      - fixing the issues invalidated the old serialized files as the end version is included in the kernel def hash
      - updated repo with serialization code
        - had to fix a few issues with code paths that worked with ORT model format only builds but not serialization build
          - added support in the full build for running transformers and Graph::Resolve on a model loaded from ORT serialized format
            - TBD if we want that - at least it's more flexible for now as it enables validating the serialize, deserialize, initialize with transformers, execute path
              - this may be required if we want to enable L3 optimizers with a deserialized model

  Validated op exclusion setup using 3 onnx model zoo models
    - re-created serialized ORT files for yolo_v3, mobilenet and mlperf ssd_mobilenet
    - executed new serialized models with deserialized build with unused ops removed and verified tests passed

  Ran operator removal script using both the OXO models as input
    - with that minimized build:
      Ort.deserialize (RelWithDebInfo.5 and MinSizeRel.5 folders)
          RelWithDebInfo  onnxruntime.dll	    1,540,608  (-14653KB)
          MinSizeRel      onnxruntime.dll	    1,307,648  (-1224.5KB)
                          onnxruntime.zip	      496,832  

  Providers: 
    400KB
      - Cast is still fairly large
      - Reducing types supported can be considered

  Framework:
    328KB
      - as per below notes a few things can be removed
      - completely removing the protobuf dependency is maybe best cost/benefit

  MLAS:
    225KB
      - Windows x64 build with AVX512 support (see below notes on size of AVX512 support)
        - not simple to exclude
        - ARM or x32 builds may be much smaller

  Session:
    157KB
      - C API 62.3KB
        - do we need this for a mobile build or is a hard dependency on the non-ABI code fine?

  Util:
    54.4KB
      - removing support for double would help

  Graph:
    49KB

  ONNX:
    25.6KB
      - only includes data_type_utils
        - could exclude ML types if disabled in build


=================================
Potential next steps:

1. DONE
  - remove optimizer support
    - -5KB for InferenceSession::TranformGraph
    - can disable a lot more code in Graph as well
    - can try building with no contrib ops to see what size is possible
      - however the disable unused ops approach is what would be needed for production given we want to save an optimized model which will require some contrib ops in most cases
    - can fully remove onnxruntime_optimizer (-46.7KB)
      - memcpy and cast transformers
      - can be removed if we're not going to change the nodes in the graph
    - can remove graph_utils (-5.7KB)
    - guesstimate ~80KB saving

2. DONE
  - excise ML types better
    - remove from data type utils etc.
    - remove from C API
      - C API is currently 100KB which is way too large 
    - guesstimate ~80KB saving

3. LATER
  - exclude AVX512 from MLAS 
    - expect -110KB
    - non-trivial to do as the setup doesn't easily allow disabling AVX512
      - estimate for saving should be accurate though as the AVX512 code is in separate files

4. DONE 
  - build with no contrib ops
    - not totally viable as per above notes - need contrib ops for optimized models but incremental growth to add those
    - saving: 

5.
  - pull in changes from android testing to disable exceptions
    - very large changeset though 
    - was ~430KB but will be smaller with more things excluded like ONNX type/shape inferencing
      - guesstimate of -250KB

6.
  - experiment with limited types
    - can get ballpark figure for a couple of models
    - assuming this will be on top of per-model/s op reductions
      - possible we could define a subset of types though to have a smaller binary with no per-model stage of a build
    - e.g. Cast (79KB), OneHot (35.5KB) and the reduction ops (140.8KB) could be much smaller  with fewer types
      - Cast can be reworked either way be closer to half the current size

7.
  - use printf formatting for all log messages
    - TF Lite takes this approach to avoid cost of convenient iostream formatting
    - onnxruntime::MakeString is a significant cost in various places
      - use by ORT_MAKE_STATUS and ORT_THROW 

8.
  - serialize allocation plan
    - allocation planner is 45KB

9.
  - make telemetry optional
    - currently only user of ModelProto metadata as well
    - and a few KB saving

10.
  - Framework is 342KB
    Should be ~100KB saving achievable from small changes
      25: remove ML support from data types   - DONE based on #define for disabling ML ops
      32: serialize allocation plan
      7: remove debug logging from BFCArena
      17: remove tensorprotoutils if we just create OrtValue instances from serialized data 
      17: parallel executor
      7: session_state_utils support for copying TensorProto to OrtValue
    ? 8: tensor_allocator (was added for Training, not sure if needed/used in inferencing)


L1. LONG TERM
  - Completely remove protobuf dependency (>100KB)
    - Currently (v2) 68KB for onnx_proto and 36KB for libprotobuf-lite
    - could also remove all the code dealing with tensor proto
      - e.g. framework:tensorprotoutils (17.5KB)
    - Would need updates to the optimizers as they deal in protobuf types for the initializers
    - This is a major change but has significant other potential benefits
      - Convert code to use offsets in the flatbuffer/flexbuffer for reading the tensor data etc. directly to reduce memory usage/copying



===================
Transformer support

ASSUMPTION: L2 can be run prior to deployment. Requires that the EPs that will be used are enabled when running the optimizations and serializing the model

Lot of cost to support L3 transformers in ORT model format build
  - If you're transforming the graph you need to be able to add new nodes
  - If adding a new node/s you need 
    - the OpSchema
    - the schema registry to store/lookup the schema
    - the lookup of kernel based on input/output types in SearchKernelRegistry
    - type/shape inferencing to run (currently has dependency on ONNX running that)
    - Graph::Resolve to run or the transformer needs to keep the graph edges valid
    - couldn't serialize allocation plan

  - significant hit to binary size to support that using existing code/approach
    - brings back in onnx dependency as well as the need for schema related code
    - ~1MB cost
  - may be possible to minimize that cost as limited number of L3 transformers (currently only NCHWc)
    - could avoid the OpSchema registry side of things if the transformer could directly specify the kernel def for the node
      - appears to be feasible as there's one KernelDef per NCHWc operator
    - type/shape inferencing has a hard dependency on ONNX
      - non-trivial to separate. may need to re-implement or calculate in ORT 
        - existing type/shape info would be available so would possibly only need to augment that. 
          - unknown if this is viable and if so whether it's trivial or not
    - keeping the edges valid so Graph::Resolve isn't required should be relatively easy

    

======================
Not supported
  - Functions
  - CustomRegistry
    - kernel registry and schema tightly bound currently
  - Optimizers

Other:
  - Could choose between serializing edges and having code to create edges (Graph::Resolve + BuildConnections)