ORT Format only build changes

High level approach:
  - Serialize Graph instance including Node instances and initializers
    - subgraphs are serialized the same way
    - current assumption is that EPs to be used in deployed version should be known, and L1 and L2 transformers can be run ahead of time 
      - we can add in transformers if this isn't the case though, but cost will vary depending on what's required
    - needed to add storage of SinceVersion in Node so ONNX OpSchema isn't required 
  - Serialize SessionState
    - currently this is just the KernelCreateInfo
    - allows kernel to be instantiated without dependency on ONNX for schema lookup/validation
      - e.g. if kernel is templatized on type this will save the KCI for the correct version
    - refactored to remove usage of SearchKernelRegistry multiple times for a Node 
      - single lookup in finalization of SessionState to save KCI
        - allocation planner and kernel registry manager now use saved KCI
    - somewhat significant refactoring so that finalization is inside SessionState and InferenceSession usage is now cleaner
      - recent cleanup of SessionState made this viable
  - Currently serializing a few things using a protobuf blob
    - done for expediency as we currently have a hard dependency on the protobuf types

Initial status from RelWithDebInfo build
  - need to build that way to get per-symbol info on Windows. MinSizeRel is signficantly smaller

=====
TODO: Need better baseline with more equivalent builds that both exclude RTTI and ML ops
=====

ONNX: 
  26KB (-800KB)
  - Library just includes data type helpers

Optimizers: 
  45KB (-277KB)
  Kept ones required post-transform (required if we enable NCHW)
    - graph tranformer, insert cast, memcpy
    - Excluded all others
    - could remove all if no transformations after load

Framework:
  346KB (-50KB)
  - Removed Custom op support
    - currently no good separation between just getting the kernel defs and the schemas
  - Removed implementation of GraphPartitioner
    - serialized model must be partitioned first
  - Remove VerifyKernelDef
  Can save in a few places:
    - 49KB in data_types and a lot of that is for ML types
    - 31KB for allocation planner: could serialize
    - 16KB parallel executor: could exclude
    - 7KB BFCArena::DumpMemoryLog + DebugString could be excluded
    - 7KB dealing with TensorProto to OrtValue - could just create OrtValue from offset in flatbuffer/flexbuffer     
      - probably a few other places where doing so would save multiple KB


Graph:
  88KB (-210KB)
  - Cut out initial set of obvious methods that weren't required. 
    - e.g. ctors used for load from proto (Model and Graph), ToProto, Serialization, Node::Op (returns onnx::OpSchema instance), OnnxRuntimeOpSchemaRegistry
  - Pieces required for transformers are still supported but could be removed
    - Could save about 20KB from that
      - e.g. Resolve, AddNode, AddEdge, RemoveEdge, etc. 
  - No support for Function 
    - tightly bound to OpSchema class in ONNX 
      - can't bring in in schema.cc from ONNX without splitting that up more
      - TBD if Function support is required
  - Can remove some training related code for some minor savings
    - consumer/provider info
  - Can remove graph_utils if no transformer support
    - only 5.6KB though

Providers:
  2.1MB (-427KB)
  *** this is slightly off as Einsum shows up as significant diff. need equivalent base commit id ***
  - Excluded ML ops which was most of diff
  - Currently includes all contrib ops
    - can split out based on category
      - Experimental/NCHWc/Optimizer related (inc. BERT specific)/
  

Session:
  280KB (-40KB)
  - Excluded custom ops and ctors involving loading ModelProto
  - Can remove support for config in model and exclude JSON parser 
    - Saving: 65KB (remove inference_session_utils.*)
  - C API could be minimized
    Currently 100KB
      - probably a lot we can remove to minimized
      - lot of cost to support ML types such as Map that could be removed based on --disable_ml_ops

MLAS:
  260KB
  - Not sure what ARM size will be
  - AVX512 support is expensive. could make that optional
    - Saving ~110KB

Common:
  79KB
  - telemetry is 6.6KB + some cost in InferenceSession
  - profiler is 7.5KB + some cost in InferenceSession
    - could be made optional

Other:
  - Exclude RE2
    - 163KB

-----

Other notes:
  - Still have a dependency on ONNX PB
    - More work to replace though given internal use of *Proto types in many places
    - Current size is 83KB
      - Could probably cut down by excluding training and function related pieces


  - Are we paying cost for templatized containers in multiple libs
    - e.g. Graph has Hash of unique_ptr<NodeArg> for 552 bytes
    - may not exist in MinSizeRel build

===============================
New setup:

Commit id from master (2020-07-08, should be ~ORT 1.4 release): 6d6b6b54a544f69fb201a9aab7097358f6a440ff
Updated: ort.vs19.perf.master, ort.serialize and ort.deserialize are all on the same base commit id

Build flags:
  Master: --disable_rtti --disable_ml_ops
  Deserialize: Add --ort_model_format 

MinSizeRel build
  Base: .\build.bat --parallel --build_shared_lib --build_wheel --cmake_generator="Visual Studio 16 2019" --config=MinSizeRel --disable_rtti --disable_ml_ops --build --update --skip_submodule_sync
  Deserialize: .\build.bat --parallel --build_shared_lib --build_wheel --cmake_generator="Visual Studio 16 2019" --config=MinSizeRel --ort_model_format --disable_rtti --disable_ml_ops --build --update --skip_submodule_sync

Master
    RelWithDebInfo  onnxruntime.dll	    5,236,736
    MinSizeRel      onnxruntime.dll	    4,398,080

Ort.deserialize (RelWithDebInfo.1 and MinSizeRel.1 folders)
    RelWithDebInfo  onnxruntime.dll	    3,833,344
    MinSizeRel      onnxruntime.dll	    3,231,744
                    onnxruntime.zip	    1,223,231
=================================

Next steps:

Reduced onnx_proto
  68KB (-15KB)
  - no usage of onnx-operators-ml.proto
  - comment out training related types
  - fix some places to not directly include schema.h from onnx as that's broken with these changes 

Removed re2 dependency and Tokenizer contrib op
  -163KB

Removed parsing of json from Model to save json parser cost
  -70KB (68 in inference_session_utils.obj and a couple in inference_session.obj) 

Ort.deserialize (RelWithDebInfo.2 and MinSizeRel.2 folders)
    RelWithDebInfo  onnxruntime.dll	    3,553,280
    MinSizeRel      onnxruntime.dll	    3,231,744
                    onnxruntime.zip	    1,223,231

=================================
Next steps:

- remove optimzer support
  - can disable a lot more code in Graph as well
  - can try building with no contrib ops to see what size is possible
    - however the disable unused ops approach is what would be needed for production given we want to save an optimized model which will require some contrib ops in most cases

- reduce onnx_proto further
  - could try and remove TensorProto dependency
    - could cut out ORT code dealing with TensorProto for a good reduction
      - heavy change to implement though
    - doing so implies no optimizers as the implementations are currently heavily dependent on working at the TensorProto level

- exclude AVS512 from MLAS 

- make telemetry optional
  - currently only user of ModelProto metadata as well
