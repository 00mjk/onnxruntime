{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.  \n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference PyTorch Bert Model for High Performance in ONNX Runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, you'll be introduced to how to load a Bert model from PyTorch, convert it to ONNX, and inference it for high performance using ONNX Runtime. In the following sections, we are going to use the Bert model trained with Stanford Question Answering Dataset (SQuAD) dataset as an example. Bert SQuAD model is used in question answering scenarios, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Prerequisites ##\n",
    "It need a python environment with [PyTorch](https://pytorch.org/) and [OnnxRuntime](https://microsoft.github.io/onnxruntime/) installed before running this notebook. \n",
    "\n",
    "First, we install [AnaConda](https://www.anaconda.com/distribution/) in a target machine and open an AnaConda prompt window when it is done. Then you can choose a setup based on your target device (CPU or GPU), and run the commands to create a conda environment.\n",
    "\n",
    "#### CPU Environment Setup\n",
    "If your machines does not have GPU or want to test CPU inference. You can create a conda environment like the following:\n",
    "\n",
    "```console\n",
    "conda create -n cpu_env python=3.6\n",
    "conda activate cpu_env\n",
    "conda install pytorch torchvision cpuonly -c pytorch\n",
    "pip install onnxruntime\n",
    "conda install jupyter\n",
    "jupyter notebook\n",
    "```\n",
    "The last command will launch Jupyter Notebook and we can open this notebook in browser to continue.\n",
    "\n",
    "Another option is to use pip to install package to your existing jupyter notebook environment:\n",
    "```console\n",
    "pip install --upgrade torch==1.4.0+cpu torchvision==0.5.0+cpu -f https://download.pytorch.org/whl/torch_stable.html\n",
    "pip install onnxruntime==1.1.2\n",
    "```\n",
    "\n",
    "#### GPU Environment Setup\n",
    "\n",
    "This requires your machine to have a GPU.\n",
    "\n",
    "```console\n",
    "conda create -n gpu_env python=3.6\n",
    "conda activate gpu_env\n",
    "conda install pytorch torchvision cudatoolkit=10.1 -c pytorch\n",
    "pip install onnxruntime-gpu\n",
    "conda install jupyter\n",
    "jupyter notebook\n",
    "```\n",
    "\n",
    "onnxruntime-gpu v1.1.2 requires installing [CUDA](https://developer.nvidia.com/cuda-downloads) 10.0 and [cuDNN](https://developer.nvidia.com/cudnn) 7.6, and add their bin directories to PATH environment variable (You need update the path in section 4 below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==2.5.1 in d:\\anaconda3\\envs\\torch14_gpu\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in d:\\anaconda3\\envs\\torch14_gpu\\lib\\site-packages (from transformers==2.5.1) (3.0.12)\n",
      "Requirement already satisfied: boto3 in d:\\anaconda3\\envs\\torch14_gpu\\lib\\site-packages (from transformers==2.5.1) (1.12.11)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\anaconda3\\envs\\torch14_gpu\\lib\\site-packages (from transformers==2.5.1) (2020.2.20)\n",
      "Requirement already satisfied: sentencepiece in d:\\anaconda3\\envs\\torch14_gpu\\lib\\site-packages (from transformers==2.5.1) (0.1.85)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\anaconda3\\envs\\torch14_gpu\\lib\\site-packages (from transformers==2.5.1) (4.43.0)\n",
      "Requirement already satisfied: sacremoses in d:\\anaconda3\\envs\\torch14_gpu\\lib\\site-packages (from transformers==2.5.1) (0.0.38)\n",
      "Requirement already satisfied: tokenizers==0.5.2 in d:\\anaconda3\\envs\\torch14_gpu\\lib\\site-packages (from transformers==2.5.1) (0.5.2)\n",
      "Requirement already satisfied: numpy in d:\\anaconda3\\envs\\torch14_gpu\\lib\\site-packages (from transformers==2.5.1) (1.18.1)\n",
      "Requirement already satisfied: requests in d:\\anaconda3\\envs\\torch14_gpu\\lib\\site-packages (from transformers==2.5.1) (2.23.0)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in d:\\anaconda3\\envs\\torch14_gpu\\lib\\site-packages (from boto3->transformers==2.5.1) (0.9.5)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in d:\\anaconda3\\envs\\torch14_gpu\\lib\\site-packages (from boto3->transformers==2.5.1) (0.3.3)\n",
      "Requirement already satisfied: botocore<1.16.0,>=1.15.11 in d:\\anaconda3\\envs\\torch14_gpu\\lib\\site-packages (from boto3->transformers==2.5.1) (1.15.11)\n",
      "Requirement already satisfied: click in d:\\anaconda3\\envs\\torch14_gpu\\lib\\site-packages (from sacremoses->transformers==2.5.1) (7.0)\n",
      "Requirement already satisfied: six in d:\\anaconda3\\envs\\torch14_gpu\\lib\\site-packages (from sacremoses->transformers==2.5.1) (1.14.0)\n",
      "Requirement already satisfied: joblib in d:\\anaconda3\\envs\\torch14_gpu\\lib\\site-packages (from sacremoses->transformers==2.5.1) (0.14.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in d:\\anaconda3\\envs\\torch14_gpu\\lib\\site-packages (from requests->transformers==2.5.1) (2.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in d:\\anaconda3\\envs\\torch14_gpu\\lib\\site-packages (from requests->transformers==2.5.1) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda3\\envs\\torch14_gpu\\lib\\site-packages (from requests->transformers==2.5.1) (2019.11.28)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in d:\\anaconda3\\envs\\torch14_gpu\\lib\\site-packages (from requests->transformers==2.5.1) (1.25.8)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in d:\\anaconda3\\envs\\torch14_gpu\\lib\\site-packages (from botocore<1.16.0,>=1.15.11->boto3->transformers==2.5.1) (2.8.1)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in d:\\anaconda3\\envs\\torch14_gpu\\lib\\site-packages (from botocore<1.16.0,>=1.15.11->boto3->transformers==2.5.1) (0.15.2)\n",
      "Requirement already satisfied: wget in d:\\anaconda3\\envs\\torch14_gpu\\lib\\site-packages (3.2)\n",
      "Requirement already satisfied: psutil in d:\\anaconda3\\envs\\torch14_gpu\\lib\\site-packages (5.7.0)\n",
      "Requirement already satisfied: onnx in d:\\anaconda3\\envs\\torch14_gpu\\lib\\site-packages (1.6.0)\n",
      "Requirement already satisfied: pytz in d:\\anaconda3\\envs\\torch14_gpu\\lib\\site-packages (2019.3)\n",
      "Requirement already satisfied: pandas in d:\\anaconda3\\envs\\torch14_gpu\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: py-cpuinfo in d:\\anaconda3\\envs\\torch14_gpu\\lib\\site-packages (5.0.0)\n",
      "Requirement already satisfied: py3nvml in d:\\anaconda3\\envs\\torch14_gpu\\lib\\site-packages (0.2.5)\n",
      "Requirement already satisfied: protobuf in d:\\anaconda3\\envs\\torch14_gpu\\lib\\site-packages (from onnx) (3.11.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.2.1 in d:\\anaconda3\\envs\\torch14_gpu\\lib\\site-packages (from onnx) (3.7.4.1)\n",
      "Requirement already satisfied: numpy in d:\\anaconda3\\envs\\torch14_gpu\\lib\\site-packages (from onnx) (1.18.1)\n",
      "Requirement already satisfied: six in d:\\anaconda3\\envs\\torch14_gpu\\lib\\site-packages (from onnx) (1.14.0)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in d:\\anaconda3\\envs\\torch14_gpu\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: xmltodict in d:\\anaconda3\\envs\\torch14_gpu\\lib\\site-packages (from py3nvml) (0.12.0)\n",
      "Requirement already satisfied: setuptools in d:\\anaconda3\\envs\\torch14_gpu\\lib\\site-packages (from protobuf->onnx) (45.2.0.post20200210)\n"
     ]
    }
   ],
   "source": [
    "# install some packages used in this notebook\n",
    "import sys\n",
    "#!{sys.executable} -m pip install --upgrade torch==1.4.0+cpu torchvision==0.5.0+cpu -f https://download.pytorch.org/whl/torch_stable.html\n",
    "#!{sys.executable} -m pip install onnxruntime==1.1.2\n",
    "!{sys.executable} -m pip install transformers==2.5.1\n",
    "!{sys.executable} -m pip install wget psutil onnx pytz pandas py-cpuinfo py3nvml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Pretrained Bert model ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by downloading the data files and store them in the specified location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "cache_dir = \"./squad\"\n",
    "if not os.path.exists(cache_dir):\n",
    "    os.makedirs(cache_dir)\n",
    "\n",
    "predict_file_url = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\"\n",
    "predict_file = os.path.join(cache_dir, \"dev-v1.1.json\")\n",
    "if not os.path.exists(predict_file):\n",
    "    import wget\n",
    "    print(\"Start downloading predict file.\")\n",
    "    wget.download(predict_file_url, predict_file)\n",
    "    print(\"Predict file downloaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify some model config variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For fine tuned large model, the model name is \"bert-large-uncased-whole-word-masking-finetuned-squad\". Here we use bert-base for demo.\n",
    "model_name_or_path = \"bert-base-cased\"\n",
    "max_seq_length = 128\n",
    "doc_stride = 128\n",
    "max_query_length = 64\n",
    "\n",
    "# enable overwrite to export onnx model and download latest script each time when running this notebook.\n",
    "enable_overwrite = False\n",
    "\n",
    "# total samples to inference, so that we can get average latency\n",
    "total_samples = 1000\n",
    "\n",
    "# onnx opset version: 10 or 11\n",
    "opset_version=11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start to load model from pretrained. This step could take a few minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 48/48 [00:03<00:00, 13.12it/s]\n",
      "convert squad examples to features: 100%|█████████████████████████████████████████| 1000/1000 [00:07<00:00, 140.04it/s]\n",
      "add example index and unique id: 100%|█████████████████████████████████████████| 1000/1000 [00:00<00:00, 983654.78it/s]\n"
     ]
    }
   ],
   "source": [
    "# The following code is adapted from HuggingFace transformers\n",
    "# https://github.com/huggingface/transformers/blob/master/examples/run_squad.py\n",
    "\n",
    "from transformers import (BertConfig, BertForQuestionAnswering, BertTokenizer)\n",
    "\n",
    "# Load pretrained model and tokenizer\n",
    "config_class, model_class, tokenizer_class = (BertConfig, BertForQuestionAnswering, BertTokenizer)\n",
    "config = config_class.from_pretrained(model_name_or_path, cache_dir=cache_dir)\n",
    "tokenizer = tokenizer_class.from_pretrained(model_name_or_path, do_lower_case=True, cache_dir=cache_dir)\n",
    "model = model_class.from_pretrained(model_name_or_path,\n",
    "                                    from_tf=False,\n",
    "                                    config=config,\n",
    "                                    cache_dir=cache_dir)\n",
    "# load some examples\n",
    "from transformers.data.processors.squad import SquadV1Processor\n",
    "\n",
    "processor = SquadV1Processor()\n",
    "examples = processor.get_dev_examples(None, filename=predict_file)\n",
    "\n",
    "from transformers import squad_convert_examples_to_features\n",
    "features, dataset = squad_convert_examples_to_features( \n",
    "            examples=examples[:total_samples], # convert just enough examples for this notebook\n",
    "            tokenizer=tokenizer,\n",
    "            max_seq_length=max_seq_length,\n",
    "            doc_stride=doc_stride,\n",
    "            max_query_length=max_query_length,\n",
    "            is_training=False,\n",
    "            return_dataset='pt'\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Export the loaded model ##\n",
    "Once the model is loaded, we can export the loaded PyTorch model to ONNX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model exported at  ./onnx\\bert-base-cased-squad_opset11.onnx\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"./onnx\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)   \n",
    "export_model_path = os.path.join(output_dir, 'bert-base-cased-squad_opset{}.onnx'.format(opset_version))\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Get the first example data to run the model and export it to ONNX\n",
    "data = dataset[0]\n",
    "inputs = {\n",
    "    'input_ids':      data[0].to(device).reshape(1, max_seq_length),\n",
    "    'attention_mask': data[1].to(device).reshape(1, max_seq_length),\n",
    "    'token_type_ids': data[2].to(device).reshape(1, max_seq_length)\n",
    "}\n",
    "\n",
    "# Set model to inference mode, which is required before exporting the model because some operators behave differently in \n",
    "# inference and training mode.\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "if enable_overwrite or not os.path.exists(export_model_path):\n",
    "    with torch.no_grad():\n",
    "        symbolic_names = {0: 'batch_size', 1: 'max_seq_len'}\n",
    "        torch.onnx.export(model,                                            # model being run\n",
    "                          args=tuple(inputs.values()),                      # model input (or a tuple for multiple inputs)\n",
    "                          f=export_model_path,                              # where to save the model (can be a file or file-like object)\n",
    "                          opset_version=opset_version,                      # the ONNX version to export the model to\n",
    "                          do_constant_folding=True,                         # whether to execute constant folding for optimization\n",
    "                          input_names=['input_ids',                         # the model's input names\n",
    "                                       'input_mask', \n",
    "                                       'segment_ids'],\n",
    "                          output_names=['start', 'end'],                    # the model's output names\n",
    "                          dynamic_axes={'input_ids': symbolic_names,        # variable length axes\n",
    "                                        'input_mask' : symbolic_names,\n",
    "                                        'segment_ids' : symbolic_names,\n",
    "                                        'start' : symbolic_names,\n",
    "                                        'end' : symbolic_names})\n",
    "        print(\"Model exported at \", export_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PyTorch Inference ##\n",
    "Use PyTorch to evaluate an example input for comparison purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch cuda Inference time = 25.39 ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Measure the latency. It is not accurate using Jupyter Notebook, it is recommended to use standalone python script.\n",
    "latency = []\n",
    "with torch.no_grad():\n",
    "    for i in range(total_samples):\n",
    "        data = dataset[i]\n",
    "        inputs = {\n",
    "            'input_ids':      data[0].to(device).reshape(1, max_seq_length),\n",
    "            'attention_mask': data[1].to(device).reshape(1, max_seq_length),\n",
    "            'token_type_ids': data[2].to(device).reshape(1, max_seq_length)\n",
    "        }\n",
    "        start = time.time()\n",
    "        outputs = model(**inputs)\n",
    "        latency.append(time.time() - start)\n",
    "print(\"PyTorch {} Inference time = {} ms\".format(device.type, format(sum(latency) * 1000 / len(latency), '.2f')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inference the Exported Model with ONNX Runtime ##\n",
    "\n",
    "To use onnxruntime-gpu, it is required to install CUDA 10.0 and CUDNN 7.6, and add their bin directories to PATH environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = (device.type == 'cuda')\n",
    "if use_gpu:\n",
    "    # Add path for CUDA 10.0 and CUDNN 7.6, which are required by onnxruntime-gpu\n",
    "    cuda_dir = 'D:/NVidia/CUDA/v10.0/bin'\n",
    "    cudnn_dir = 'D:/NVidia/CUDA/v10.0/bin'\n",
    "    if not (os.path.exists(cuda_dir) and os.path.exists(cudnn_dir)):\n",
    "        raise ValueError(\"Please specify correct path for CUDA 10.0 and CUDNN 7.6. Otherwise onnxruntime-gpu cannot be imported.\")\n",
    "    else:\n",
    "        if cuda_dir == cudnn_dir:\n",
    "            os.environ[\"PATH\"] = cuda_dir + ';' + os.environ[\"PATH\"]\n",
    "        else:\n",
    "            os.environ[\"PATH\"] = cuda_dir + ';' + cudnn_dir + ';' + os.environ[\"PATH\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "\n",
    "# You can change it to False if Performance Test Tool shows that intra_op_num_threads > 1 could achieve better result.\n",
    "use_openmp = False\n",
    "\n",
    "# onnxruntim-gpu is not built with OpenMP so no need to set OpenMP envirnoment variables\n",
    "if use_gpu:\n",
    "    use_openmp = False\n",
    "    \n",
    "# The following setups OpenMP environment variables. For best setting, see the section of Performance Test Tool below.\n",
    "# ATTENTION: these environment variables must be set before importing onnxruntime. Otherwise, they will not take effect.\n",
    "if use_openmp:\n",
    "    os.environ[\"OMP_NUM_THREADS\"] = str(psutil.cpu_count(logical=True))\n",
    "else:\n",
    "    os.environ[\"OMP_NUM_THREADS\"] = '1'\n",
    "\n",
    "# You may change it to ACTIVE if Performance Test Tool shows that it could be better.\n",
    "os.environ[\"OMP_WAIT_POLICY\"] = 'PASSIVE'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to inference the model with ONNX Runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OnnxRuntime cuda Inference time = 8.51 ms\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "import onnxruntime\n",
    "import numpy\n",
    "\n",
    "device_name = 'cuda' if 'CUDAExecutionProvider' in onnxruntime.get_available_providers() else 'cpu'\n",
    "if use_gpu:\n",
    "    assert device_name == \"cuda\"\n",
    "   \n",
    "sess_options = onnxruntime.SessionOptions()\n",
    "\n",
    "# Optional: store the optimized graph and view it using Netron to verify that model is fully optimized.\n",
    "# Note that this will increase session creation time so enable it for debugging only.\n",
    "# sess_options.optimized_model_filepath = os.path.join(output_dir, \"optimized_model_{}.onnx\".format(device_name))\n",
    "   \n",
    "if use_openmp:\n",
    "    sess_options.intra_op_num_threads=1\n",
    "else:\n",
    "    sess_options.intra_op_num_threads=psutil.cpu_count(logical=True)\n",
    "\n",
    "session = onnxruntime.InferenceSession(export_model_path, sess_options)\n",
    "\n",
    "latency = []\n",
    "for i in range(total_samples):\n",
    "    data = dataset[i]\n",
    "    # Use contiguous array as input might improve performance\n",
    "    ort_inputs = {\n",
    "        'input_ids':  numpy.ascontiguousarray(data[0].cpu().reshape(1, max_seq_length).numpy()),\n",
    "        'input_mask': numpy.ascontiguousarray(data[1].cpu().reshape(1, max_seq_length).numpy()),\n",
    "        'segment_ids': numpy.ascontiguousarray(data[2].cpu().reshape(1, max_seq_length).numpy())\n",
    "    }\n",
    "    start = time.time()\n",
    "    ort_outputs = session.run(None, ort_inputs)\n",
    "    latency.append(time.time() - start)\n",
    "print(\"OnnxRuntime {} Inference time = {} ms\".format(device_name, format(sum(latency) * 1000 / len(latency), '.2f')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Verifying correctness *****\n",
      "PyTorch and ONNX Runtime output 0 are close: True\n",
      "PyTorch and ONNX Runtime output 1 are close: True\n"
     ]
    }
   ],
   "source": [
    "print(\"***** Verifying correctness *****\")\n",
    "for i in range(2):\n",
    "    print('PyTorch and ONNX Runtime output {} are close:'.format(i), numpy.allclose(ort_outputs[i], outputs[i].cpu(), rtol=1e-05, atol=1e-04))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference with Actual Sequence Length\n",
    "Note that ONNX model is exported using dynamic length axis. A simple trick for better performance is to dynamic length input instead of fixed length input. Let's see how it can be applied to this model.\n",
    "\n",
    "**Note**: Need evaluate the end-to-end impact on performance and accuracy if you use this strategy. It could be a good trade off for some senario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1293,  1242,  2557,  1127,  1226,  1104,  1103,  3613, 16429,\n",
       "           5235,   136,   102,  3613, 16429,  5988,   170,   107,  1353,  1671,\n",
       "           1992,  1342,   107,  5235,   117,  1107,  1134,  1473,  3683,  3538,\n",
       "           1125,   170,  1476,   118,  1248,  2595,  4086,  1714,  1104,  2965,\n",
       "          15897,  1104,  3613, 16429,   119,  1473,  3683,  3538,  3222,  1149,\n",
       "           2551,  1168, 23759,  1116,  1121,  1506,  1103, 10280,  2231,  1111,\n",
       "           1103,  1714, 16355,   119,   102,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0]],\n",
       "        device='cuda:0'),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0'),\n",
       " 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# An example input (we can see padding). From attention_mask, we can deduce the actual length.\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length 101\n",
      "OnnxRuntime cuda Inference time with actual sequence length = 7.45 ms\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "\n",
    "latency = []\n",
    "lengths = []\n",
    "for i in range(total_samples):\n",
    "    data = dataset[i]\n",
    "    # Instead of using fixed length (128), we can use actual sequence length (less than 128), which helps to get better performance.\n",
    "    actual_sequence_length = sum(data[1].numpy())\n",
    "    lengths.append(actual_sequence_length)\n",
    "    opt_inputs = {\n",
    "        'input_ids':  data[0].numpy()[:actual_sequence_length].reshape(1, actual_sequence_length),\n",
    "        'input_mask': data[1].numpy()[:actual_sequence_length].reshape(1, actual_sequence_length),\n",
    "        'segment_ids': data[2].numpy()[:actual_sequence_length].reshape(1, actual_sequence_length)\n",
    "    }\n",
    "    start = time.time()\n",
    "    opt_outputs = session.run(None, opt_inputs)\n",
    "    latency.append(time.time() - start)\n",
    "print(\"Average length\", statistics.mean(lengths))\n",
    "print(\"OnnxRuntime {} Inference time with actual sequence length = {} ms\".format(device_name, format(sum(latency) * 1000 / len(latency), '.2f')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Verifying correctness for using actual sequence length *****\n",
      "PyTorch and ONNX Runtime output 0 are close: True\n",
      "PyTorch and ONNX Runtime output 1 are close: True\n"
     ]
    }
   ],
   "source": [
    "print(\"***** Verifying correctness for using actual sequence length *****\")\n",
    "for i in range(2):\n",
    "    print('PyTorch and ONNX Runtime output {} are close:'.format(i), numpy.allclose(opt_outputs[i], outputs[i].cpu()[:,:len(opt_outputs[i][0])], rtol=1e-05, atol=1e-04))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Offline Optimization Script and Test Tools\n",
    "For better control of optimization, accuracy & performance tests, it is recommended to download the [OnnxRuntime Python Tools for BERT](https://github.com/microsoft/onnxruntime/tree/master/onnxruntime/python/tools/bert), and try them on the exported ONNX models.\n",
    "\n",
    "You may copy the whole [directory](https://github.com/microsoft/onnxruntime/tree/master/onnxruntime/python/tools/bert) to a sub-directory named bert_scripts. Here is a list of script files needed at the time this notebook created. The list need update if import error happens when you run some script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wget\n",
    "url_prfix = \"https://raw.githubusercontent.com/microsoft/onnxruntime/bert_notebooks/onnxruntime/python/tools/bert/\"\n",
    "script_files = ['bert_perf_test.py', 'bert_test_data.py', 'compare_bert_results.py', 'BertOnnxModel.py', 'BertOnnxModelKeras.py', 'BertOnnxModelTF.py', 'OnnxModel.py', 'bert_model_optimization.py']\n",
    "\n",
    "script_dir = './bert_scripts'\n",
    "if not os.path.exists(script_dir):\n",
    "    os.makedirs(script_dir)\n",
    "\n",
    "for filename in script_files:\n",
    "    target_file = os.path.join(script_dir, filename)\n",
    "    if enable_overwrite and os.path.exists(target_file):\n",
    "        os.remove(target_file)\n",
    "    if not os.path.exists(target_file):\n",
    "        wget.download(url_prfix + filename, target_file)\n",
    "        print(\"Downloaded\", filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERT Optimization Script\n",
    "\n",
    "Although OnnxRuntime could optimize Bert model exported by PyTorch. Sometime, model cannot be fully optimized due to different reasons:\n",
    "* A new subgraph pattern is generated by training and export tool, and the pattern is not covered by released onnxruntime package.\n",
    "* The exported model uses dynamic axis and this makes it harder for shape inference of the graph. That blocks some optimization to be applied.\n",
    "* Some optimization is better to be done offline. Like change input tensor type from int64 to int32 to avoid extra Cast nodes, or convert model to float16 to achieve better performance in V100 or T4 GPU.\n",
    "\n",
    "We have python script **bert_model_optimization.py**, which is more flexible in graph pattern matching and model conversion (like float32 to float16). You can also use it to verify that whether a Bert model is fully optimized.\n",
    "\n",
    "In this example, we can see that it introduces more optimizations that is not provided by onnxruntime:\n",
    "* Gelu from PyTorch 1.4 is not fused by OnnxRuntime 1.1.2. (Note: it is covered in onnxruntime master branch)\n",
    "* SkipLayerNormalization and bias (Add) are not fused in OnnxRuntime due to shape inference as mentioned.\n",
    "\n",
    "The tool will tell whether a model is fully optimized or not. If not, that means you might need change the script to handle some new patern of subgraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_model_optimization.py: Save optimized model by onnxruntime to ./onnx\\bert-base-cased-squad_opset11_ort_gpu.onnx\n",
      "bert_model_optimization.py: Use OnnxRuntime to optimize and save the optimized model to ./onnx\\bert-base-cased-squad_opset11_ort_gpu.onnx\n",
      "    BertOnnxModel.py: Fused LayerNormalization count: 0\n",
      "    BertOnnxModel.py: Fused FastGelu (approximation) count:12\n",
      "    BertOnnxModel.py: Fused Reshape count:0\n",
      "    BertOnnxModel.py: Fused SkipLayerNormalization count: 24\n",
      "    BertOnnxModel.py: Fused Attention count:0\n",
      "    BertOnnxModel.py: skip embed layer fusion since mask input is not found\n",
      "    BertOnnxModel.py: Fused FastGelu with Bias count:12\n",
      "    BertOnnxModel.py: Fused SkipLayerNormalization with Bias count:24\n",
      "    BertOnnxModel.py: opset verion: 11\n",
      "        OnnxModel.py: Output model to ./onnx/bert-base-cased-squad_opt_gpu.onnx\n",
      "    BertOnnxModel.py: EmbedLayer=1, Attention=12, Gelu=12, LayerNormalization=24, Succesful=True\n",
      "bert_model_optimization.py: The output model is fully optimized.\n"
     ]
    }
   ],
   "source": [
    "GPU_OPTION = '--gpu_only' if use_gpu else ''\n",
    "optimized_model_path = './onnx/bert-base-cased-squad_opt_{}.onnx'.format('gpu' if use_gpu else 'cpu')\n",
    "%run ./bert_scripts/bert_model_optimization.py --input $export_model_path --output $optimized_model_path $GPU_OPTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimized Graph\n",
    "Run the next step, we will open the optimized model using Netron to visualize. The graph is like the following:\n",
    "<img src="images/optimized_bert_graph.png">\n",
    "\n",
    "Sometime, optimized graph is slightly different: for example, FastGelu is replaced by BiasGelu for CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: netron in d:\\anaconda3\\envs\\torch14_gpu\\lib\\site-packages (3.9.6)\n",
      "Serving './onnx/bert-base-cased-squad_opt_gpu.onnx' at http://localhost:8080\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install netron\n",
    "import netron\n",
    "netron.start(optimized_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance Test Tool\n",
    "\n",
    "The following will create 1000 random inputs of batch_size 1 and sequence length 128, then measure the average latency and throughput numbers.\n",
    "\n",
    "Note that the test uses fixed sequence length. If you use actual sequence length, the performance could be better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating test data...\n",
      "Extra latency for converting inputs to contiguous: 0.01 ms\n",
      "Test summary is saved to onnx\\perf_results_GPU_B1_S128_20200310-031647.txt\n"
     ]
    }
   ],
   "source": [
    "GPU_OPTION = '--use_gpu' if use_gpu else ''\n",
    "\n",
    "%run ./bert_scripts/bert_perf_test.py --model $optimized_model_path --batch_size 1 --sequence_length 128 --samples 1000 --test_times 1 --inclusive --all $GPU_OPTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's loaded the summary file and take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./onnx\\perf_results_GPU_B1_S128_20200310-031647.txt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Latency(ms)</th>\n",
       "      <th>Throughput(QPS)</th>\n",
       "      <th>intra_op_num_threads</th>\n",
       "      <th>OMP_NUM_THREADS</th>\n",
       "      <th>OMP_WAIT_POLICY</th>\n",
       "      <th>contiguous</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.92</td>\n",
       "      <td>144.48</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.93</td>\n",
       "      <td>144.29</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.94</td>\n",
       "      <td>144.15</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.95</td>\n",
       "      <td>143.98</td>\n",
       "      <td>12</td>\n",
       "      <td>12.0</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.95</td>\n",
       "      <td>143.78</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.96</td>\n",
       "      <td>143.69</td>\n",
       "      <td>1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>PASSIVE</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6.98</td>\n",
       "      <td>143.36</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6.98</td>\n",
       "      <td>143.25</td>\n",
       "      <td>12</td>\n",
       "      <td>6.0</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6.98</td>\n",
       "      <td>143.21</td>\n",
       "      <td>12</td>\n",
       "      <td>12.0</td>\n",
       "      <td>PASSIVE</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6.98</td>\n",
       "      <td>143.17</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6.99</td>\n",
       "      <td>143.02</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>PASSIVE</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>7.00</td>\n",
       "      <td>142.91</td>\n",
       "      <td>12</td>\n",
       "      <td>6.0</td>\n",
       "      <td>PASSIVE</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>7.03</td>\n",
       "      <td>142.15</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>7.04</td>\n",
       "      <td>142.06</td>\n",
       "      <td>6</td>\n",
       "      <td>12.0</td>\n",
       "      <td>PASSIVE</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>7.04</td>\n",
       "      <td>142.03</td>\n",
       "      <td>6</td>\n",
       "      <td>12.0</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>7.06</td>\n",
       "      <td>141.72</td>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>7.10</td>\n",
       "      <td>140.93</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>PASSIVE</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>7.13</td>\n",
       "      <td>140.31</td>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>PASSIVE</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>7.15</td>\n",
       "      <td>139.84</td>\n",
       "      <td>1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>7.16</td>\n",
       "      <td>139.57</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>7.18</td>\n",
       "      <td>139.37</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>7.19</td>\n",
       "      <td>139.15</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>7.20</td>\n",
       "      <td>138.95</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>PASSIVE</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>7.23</td>\n",
       "      <td>138.38</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>7.23</td>\n",
       "      <td>138.28</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>PASSIVE</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>7.25</td>\n",
       "      <td>138.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>7.25</td>\n",
       "      <td>137.91</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>7.28</td>\n",
       "      <td>137.45</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>7.30</td>\n",
       "      <td>137.00</td>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>7.30</td>\n",
       "      <td>136.93</td>\n",
       "      <td>12</td>\n",
       "      <td>12.0</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>7.31</td>\n",
       "      <td>136.79</td>\n",
       "      <td>6</td>\n",
       "      <td>12.0</td>\n",
       "      <td>PASSIVE</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>7.32</td>\n",
       "      <td>136.53</td>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>PASSIVE</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>7.32</td>\n",
       "      <td>136.55</td>\n",
       "      <td>12</td>\n",
       "      <td>6.0</td>\n",
       "      <td>PASSIVE</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>7.34</td>\n",
       "      <td>136.25</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>PASSIVE</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>7.36</td>\n",
       "      <td>135.94</td>\n",
       "      <td>1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>7.36</td>\n",
       "      <td>135.79</td>\n",
       "      <td>12</td>\n",
       "      <td>12.0</td>\n",
       "      <td>PASSIVE</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>7.40</td>\n",
       "      <td>135.10</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>PASSIVE</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>7.42</td>\n",
       "      <td>134.81</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>7.47</td>\n",
       "      <td>133.96</td>\n",
       "      <td>12</td>\n",
       "      <td>6.0</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>7.58</td>\n",
       "      <td>131.91</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>PASSIVE</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>7.60</td>\n",
       "      <td>131.66</td>\n",
       "      <td>1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>PASSIVE</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>7.61</td>\n",
       "      <td>131.36</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>7.63</td>\n",
       "      <td>131.03</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>PASSIVE</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>7.66</td>\n",
       "      <td>130.62</td>\n",
       "      <td>6</td>\n",
       "      <td>12.0</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Latency(ms)  Throughput(QPS)  intra_op_num_threads  OMP_NUM_THREADS  \\\n",
       "0          6.92           144.48                     1              1.0   \n",
       "1          6.93           144.29                    12              NaN   \n",
       "2          6.94           144.15                     1              6.0   \n",
       "3          6.95           143.98                    12             12.0   \n",
       "4          6.95           143.78                    12              1.0   \n",
       "5          6.96           143.69                     1             12.0   \n",
       "6          6.98           143.36                     1              NaN   \n",
       "7          6.98           143.25                    12              6.0   \n",
       "8          6.98           143.21                    12             12.0   \n",
       "9          6.98           143.17                     6              1.0   \n",
       "10         6.99           143.02                     6              1.0   \n",
       "11         7.00           142.91                    12              6.0   \n",
       "12         7.03           142.15                     6              NaN   \n",
       "13         7.04           142.06                     6             12.0   \n",
       "14         7.04           142.03                     6             12.0   \n",
       "15         7.06           141.72                     6              6.0   \n",
       "16         7.10           140.93                     1              6.0   \n",
       "17         7.13           140.31                     6              6.0   \n",
       "18         7.15           139.84                     1             12.0   \n",
       "19         7.16           139.57                     0              NaN   \n",
       "20         7.18           139.37                     0              NaN   \n",
       "21         7.19           139.15                     1              NaN   \n",
       "22         7.20           138.95                    12              1.0   \n",
       "23         7.23           138.38                    12              NaN   \n",
       "24         7.23           138.28                     1              1.0   \n",
       "25         7.25           138.00                     1              1.0   \n",
       "26         7.25           137.91                     1              6.0   \n",
       "27         7.28           137.45                     6              NaN   \n",
       "28         7.30           137.00                     6              6.0   \n",
       "29         7.30           136.93                    12             12.0   \n",
       "30         7.31           136.79                     6             12.0   \n",
       "32         7.32           136.53                     6              6.0   \n",
       "31         7.32           136.55                    12              6.0   \n",
       "33         7.34           136.25                     1              6.0   \n",
       "34         7.36           135.94                     1             12.0   \n",
       "35         7.36           135.79                    12             12.0   \n",
       "36         7.40           135.10                    12              1.0   \n",
       "37         7.42           134.81                     6              1.0   \n",
       "38         7.47           133.96                    12              6.0   \n",
       "39         7.58           131.91                     1              1.0   \n",
       "40         7.60           131.66                     1             12.0   \n",
       "41         7.61           131.36                    12              1.0   \n",
       "42         7.63           131.03                     6              1.0   \n",
       "43         7.66           130.62                     6             12.0   \n",
       "\n",
       "   OMP_WAIT_POLICY  contiguous  \n",
       "0           ACTIVE       False  \n",
       "1              NaN       False  \n",
       "2           ACTIVE       False  \n",
       "3           ACTIVE       False  \n",
       "4           ACTIVE       False  \n",
       "5          PASSIVE       False  \n",
       "6              NaN       False  \n",
       "7           ACTIVE       False  \n",
       "8          PASSIVE       False  \n",
       "9           ACTIVE       False  \n",
       "10         PASSIVE       False  \n",
       "11         PASSIVE       False  \n",
       "12             NaN       False  \n",
       "13         PASSIVE       False  \n",
       "14          ACTIVE       False  \n",
       "15          ACTIVE       False  \n",
       "16         PASSIVE       False  \n",
       "17         PASSIVE       False  \n",
       "18          ACTIVE       False  \n",
       "19             NaN        True  \n",
       "20             NaN       False  \n",
       "21             NaN        True  \n",
       "22         PASSIVE       False  \n",
       "23             NaN        True  \n",
       "24         PASSIVE       False  \n",
       "25          ACTIVE        True  \n",
       "26          ACTIVE        True  \n",
       "27             NaN        True  \n",
       "28          ACTIVE        True  \n",
       "29          ACTIVE        True  \n",
       "30         PASSIVE        True  \n",
       "32         PASSIVE        True  \n",
       "31         PASSIVE        True  \n",
       "33         PASSIVE        True  \n",
       "34          ACTIVE        True  \n",
       "35         PASSIVE        True  \n",
       "36         PASSIVE        True  \n",
       "37          ACTIVE        True  \n",
       "38          ACTIVE        True  \n",
       "39         PASSIVE        True  \n",
       "40         PASSIVE        True  \n",
       "41          ACTIVE        True  \n",
       "42         PASSIVE        True  \n",
       "43          ACTIVE        True  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import glob     \n",
    "import pandas\n",
    "latest_result_file = max(glob.glob(\"./onnx/perf_results_*.txt\"), key=os.path.getmtime)\n",
    "result_data = pandas.read_table(latest_result_file)\n",
    "print(latest_result_file)\n",
    "sorted_result = result_data.sort_values(by='Latency(ms)', ascending=True)\n",
    "# Remove some columns that have same values for all rows.\n",
    "sorted_result.drop(['model', 'graph_optimization_level', 'batch_size', 'sequence_length', 'test_cases', 'test_times', 'use_gpu'], axis=1, inplace=True)\n",
    "sorted_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a BERT model is optimized, some approximation is used in calculation. If your BERT model has three inputs, a script compare_bert_results.py can be used to do a quick verification. The tool will generate some fake input data, and compare the inference outputs of the original and optimized models. If outputs are all close, it is safe to use the optimized model.\n",
    "\n",
    "For GPU inference, the absolute or relative difference is larger than those numbers of CPU inference. Note that slight differce in output will not impact final result. We did end-to-end evaluation using SQuAD data set using a fine-tuned squad model, and F1 score is almost the same before/after optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% passed for 100 random inputs given thresholds (rtol=0.01, atol=0.01).\n",
      "maximum absolute difference=0.008397899568080902\n",
      "maximum relative difference=51.066959381103516\n"
     ]
    }
   ],
   "source": [
    "%run ./bert_scripts/compare_bert_results.py --baseline_model $export_model_path --optimized_model $optimized_model_path --batch_size 1 --sequence_length 128 --samples 100 --rtol 0.01 --atol 0.01 $GPU_OPTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Info\n",
    "Below are some additional information of machine, and dump of outputs from PyTorch and OnnxRuntime.\n",
    "\n",
    "Here is the machine configuration that generated the above results. You might get slower or faster result according to your hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"gpu\": {\n",
      "    \"driver_version\": \"441.22\",\n",
      "    \"devices\": [\n",
      "      {\n",
      "        \"memory_total\": 8589934592,\n",
      "        \"memory_available\": 2519916544,\n",
      "        \"name\": \"GeForce GTX 1070\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  \"cpu\": {\n",
      "    \"brand\": \"Intel(R) Core(TM) i7-8700 CPU @ 3.20GHz\",\n",
      "    \"cores\": 6,\n",
      "    \"logical_cores\": 12,\n",
      "    \"hz\": \"3.1920 GHz\",\n",
      "    \"l2_cache_size\": \"1536 KB\",\n",
      "    \"l3_cache_size\": \"12288 KB\",\n",
      "    \"processor\": \"Intel64 Family 6 Model 158 Stepping 10, GenuineIntel\"\n",
      "  },\n",
      "  \"memory\": {\n",
      "    \"total\": 16971259904,\n",
      "    \"available\": 3281821696\n",
      "  },\n",
      "  \"python\": \"3.6.10.final.0 (64 bit)\",\n",
      "  \"os\": \"Windows-10-10.0.18362-SP0\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "%run ./bert_scripts/machine_info.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.2759,  0.2198,  0.2541, -0.0816,  0.0167,  0.2504,  0.1627,  0.2406,\n",
       "          -0.1095, -0.3934, -0.0827, -0.2164,  0.6624, -0.0616, -0.3850, -0.1060,\n",
       "           0.0694,  0.0561,  0.3356, -0.2215,  0.2502, -0.0965,  0.0123, -0.0625,\n",
       "          -0.2031,  0.1300,  0.2116,  0.4848, -0.1087, -0.2422,  0.0614,  0.1560,\n",
       "           0.0175,  0.1928,  0.2603,  0.2391,  0.0117, -0.0608, -0.1465, -0.2062,\n",
       "          -0.0984,  0.0549, -0.0695, -0.2939,  0.2956,  0.4645, -0.1082, -0.2422,\n",
       "          -0.2101, -0.1167, -0.1848, -0.1672,  0.0048, -0.1938, -0.0803,  0.2772,\n",
       "           0.2413,  0.0692,  0.1657, -0.2597,  0.1916, -0.0347,  0.2369,  0.6559,\n",
       "           0.6624, -0.1447, -0.1851, -0.1611, -0.0979, -0.0994, -0.0574, -0.0266,\n",
       "          -0.0089, -0.0893, -0.1150, -0.0498, -0.0910, -0.1564, -0.1301, -0.1276,\n",
       "          -0.0561, -0.1303, -0.0037,  0.1073,  0.0440,  0.0077,  0.0500, -0.0318,\n",
       "           0.1471, -0.1297, -0.0979, -0.1001, -0.0716, -0.1193, -0.1568, -0.1414,\n",
       "          -0.1431, -0.1151, -0.1631, -0.0619, -0.0636, -0.0302, -0.1036, -0.1155,\n",
       "          -0.0493,  0.1039, -0.0934, -0.1133, -0.1043, -0.1301,  0.1021,  0.0393,\n",
       "           0.0084, -0.1555, -0.1164, -0.1128, -0.1356, -0.1080, -0.1487, -0.1372,\n",
       "          -0.2153, -0.1502, -0.2023, -0.2913, -0.2677, -0.0882,  0.0117, -0.0815]],\n",
       "        device='cuda:0'),\n",
       " tensor([[ 1.6302e-01, -2.5716e-02,  2.5966e-01,  4.0141e-01,  2.7449e-02,\n",
       "           2.9826e-01,  2.0115e-01,  1.4617e-01,  7.5285e-02,  1.1700e-01,\n",
       "           1.7702e-01,  4.8297e-01,  1.2477e-01,  1.6743e-01, -7.9536e-02,\n",
       "           3.7543e-01,  6.8098e-06,  1.2038e-01, -2.0494e-02, -4.6874e-02,\n",
       "           3.3753e-01, -2.0019e-01, -4.9268e-02,  4.2258e-02,  2.1631e-01,\n",
       "           1.2156e-01, -2.1063e-01, -1.3970e-01, -3.2956e-01,  1.3421e-01,\n",
       "           2.8518e-02, -3.2658e-02,  3.5256e-01,  3.4341e-02,  2.2607e-01,\n",
       "           1.6066e-01,  2.7856e-01,  1.9802e-01, -4.5606e-01, -6.9820e-02,\n",
       "           3.8785e-01,  1.0930e-03,  1.3758e-02, -3.6182e-02, -2.5346e-01,\n",
       "          -2.3822e-01, -4.1653e-01,  7.8006e-02,  1.7988e-01,  2.5529e-02,\n",
       "          -1.7091e-01, -1.1986e-01,  6.6057e-02,  1.0869e-02, -3.3319e-01,\n",
       "           2.2435e-01, -1.2437e-01, -1.6952e-01,  4.0780e-01, -5.6073e-02,\n",
       "           7.3321e-02,  1.7746e-01,  3.3911e-01,  1.2420e-01,  1.2477e-01,\n",
       "           1.1688e-01,  2.5641e-01,  1.4953e-01,  1.0546e-01,  6.4240e-02,\n",
       "           1.0272e-01,  2.5058e-02,  6.0897e-02,  1.7362e-01,  2.0441e-01,\n",
       "           8.2591e-02,  4.6120e-02,  7.1093e-02,  1.0978e-01,  1.1045e-01,\n",
       "          -1.1109e-01,  1.0121e-01,  1.2253e-01,  3.0004e-01,  2.6173e-01,\n",
       "           2.0365e-01,  2.2299e-01,  3.7754e-02, -1.1576e-01,  9.6495e-02,\n",
       "           9.0534e-02,  2.2408e-01,  1.3028e-01, -3.0886e-02,  1.4704e-02,\n",
       "           7.1261e-03,  1.9398e-02,  1.8854e-01,  8.8337e-02, -1.2966e-02,\n",
       "           1.1960e-02,  1.4702e-01,  3.0564e-01,  2.0903e-01,  2.0717e-01,\n",
       "          -1.4001e-02,  1.2429e-01,  2.4703e-01,  2.5423e-01,  1.5457e-01,\n",
       "          -6.7889e-02,  1.2040e-01,  2.2624e-01,  2.3152e-01,  1.7037e-01,\n",
       "          -3.5275e-02,  4.5911e-02,  8.5031e-02,  1.1393e-01,  1.3376e-01,\n",
       "           1.7102e-01,  1.6911e-01,  2.8409e-01,  3.2679e-01,  1.9341e-01,\n",
       "           1.7782e-01, -2.7756e-03,  6.5696e-02]], device='cuda:0'))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PyTorch output\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.27594674,  0.21984822,  0.25413558, -0.08162472,  0.01668325,\n",
       "          0.25038725,  0.16266353,  0.24064493, -0.10954297, -0.3933645 ,\n",
       "         -0.08273189, -0.21643178,  0.6624239 , -0.06160849, -0.38500285,\n",
       "         -0.10603996,  0.0693749 ,  0.05612227,  0.33561486, -0.22150147,\n",
       "          0.2502498 , -0.09646852,  0.01226454, -0.06253117, -0.20306084,\n",
       "          0.13001284,  0.21155132,  0.48482364, -0.10873769, -0.24222723,\n",
       "          0.0613767 ,  0.15596685,  0.01754492,  0.19280124,  0.26034456,\n",
       "          0.23908499,  0.01173354, -0.06080984, -0.14651725, -0.20623967,\n",
       "         -0.09836124,  0.05493683, -0.06952707, -0.2938555 ,  0.2956541 ,\n",
       "          0.46449935, -0.10821079, -0.24217278, -0.21009654, -0.11673632,\n",
       "         -0.18481727, -0.16723992,  0.00483879, -0.19379617, -0.08032098,\n",
       "          0.2771835 ,  0.2413331 ,  0.06920868,  0.16573788, -0.25973213,\n",
       "          0.19156355, -0.03467644,  0.2368511 ,  0.6559181 ,  0.66242325,\n",
       "         -0.1447145 , -0.18511651, -0.16109265, -0.09788909, -0.0993572 ,\n",
       "         -0.0574252 , -0.02663878, -0.0089209 , -0.08933507, -0.11501521,\n",
       "         -0.04976104, -0.09099539, -0.15639171, -0.13008069, -0.12761897,\n",
       "         -0.05607423, -0.13031363, -0.00371075,  0.10726678,  0.0440142 ,\n",
       "          0.00767037,  0.04997505, -0.03179022,  0.14711308, -0.1296853 ,\n",
       "         -0.09794202, -0.10007113, -0.07163482, -0.11929785, -0.15684146,\n",
       "         -0.14138371, -0.14311229, -0.11505833, -0.16313311, -0.06189763,\n",
       "         -0.06361384, -0.03020597, -0.10357087, -0.11552203, -0.04933339,\n",
       "          0.10387746, -0.09344975, -0.11332537, -0.1042532 , -0.13014829,\n",
       "          0.10209297,  0.03930864,  0.00844713, -0.15551595, -0.11644414,\n",
       "         -0.11281698, -0.1356169 , -0.10800435, -0.14868096, -0.13721974,\n",
       "         -0.21533147, -0.15022814, -0.2023237 , -0.29130998, -0.26774937,\n",
       "         -0.08818643,  0.01167747, -0.08146235]], dtype=float32),\n",
       " array([[ 1.63020894e-01, -2.57163197e-02,  2.59656280e-01,\n",
       "          4.01406050e-01,  2.74486244e-02,  2.98260540e-01,\n",
       "          2.01149955e-01,  1.46165669e-01,  7.52854049e-02,\n",
       "          1.16999894e-01,  1.77018180e-01,  4.82967764e-01,\n",
       "          1.24772936e-01,  1.67430758e-01, -7.95363188e-02,\n",
       "          3.75430286e-01,  6.94021583e-06,  1.20381914e-01,\n",
       "         -2.04938725e-02, -4.68738526e-02,  3.37533891e-01,\n",
       "         -2.00187013e-01, -4.92678657e-02,  4.22583595e-02,\n",
       "          2.16307238e-01,  1.21557146e-01, -2.10626632e-01,\n",
       "         -1.39703423e-01, -3.29561442e-01,  1.34205684e-01,\n",
       "          2.85185277e-02, -3.26583609e-02,  3.52557868e-01,\n",
       "          3.43410820e-02,  2.26066127e-01,  1.60656899e-01,\n",
       "          2.78563648e-01,  1.98015019e-01, -4.56058651e-01,\n",
       "         -6.98197484e-02,  3.87853652e-01,  1.09281391e-03,\n",
       "          1.37577653e-02, -3.61822248e-02, -2.53462017e-01,\n",
       "         -2.38223180e-01, -4.16534126e-01,  7.80062005e-02,\n",
       "          1.79880649e-01,  2.55289376e-02, -1.70910865e-01,\n",
       "         -1.19858682e-01,  6.60568029e-02,  1.08686984e-02,\n",
       "         -3.33188325e-01,  2.24351555e-01, -1.24365553e-01,\n",
       "         -1.69521183e-01,  4.07802641e-01, -5.60722873e-02,\n",
       "          7.33210593e-02,  1.77458435e-01,  3.39106202e-01,\n",
       "          1.24202237e-01,  1.24771863e-01,  1.16876341e-01,\n",
       "          2.56405294e-01,  1.49528533e-01,  1.05455004e-01,\n",
       "          6.42395318e-02,  1.02723815e-01,  2.50583142e-02,\n",
       "          6.08975142e-02,  1.73616871e-01,  2.04412058e-01,\n",
       "          8.25903863e-02,  4.61201817e-02,  7.10931197e-02,\n",
       "          1.09780088e-01,  1.10451765e-01, -1.11091383e-01,\n",
       "          1.01209141e-01,  1.22533917e-01,  3.00044328e-01,\n",
       "          2.61734337e-01,  2.03653187e-01,  2.22987086e-01,\n",
       "          3.77543308e-02, -1.15760341e-01,  9.64953005e-02,\n",
       "          9.05338302e-02,  2.24078745e-01,  1.30282283e-01,\n",
       "         -3.08864005e-02,  1.47038586e-02,  7.12611899e-03,\n",
       "          1.93981007e-02,  1.88541099e-01,  8.83373171e-02,\n",
       "         -1.29662752e-02,  1.19597614e-02,  1.47017807e-01,\n",
       "          3.05639386e-01,  2.09031761e-01,  2.07173571e-01,\n",
       "         -1.40016079e-02,  1.24291122e-01,  2.47030959e-01,\n",
       "          2.54231721e-01,  1.54567957e-01, -6.78888485e-02,\n",
       "          1.20397091e-01,  2.26241440e-01,  2.31520832e-01,\n",
       "          1.70373976e-01, -3.52753699e-02,  4.59105372e-02,\n",
       "          8.50309283e-02,  1.13925815e-01,  1.33761868e-01,\n",
       "          1.71024919e-01,  1.69105217e-01,  2.84090877e-01,\n",
       "          3.26794565e-01,  1.93406612e-01,  1.77815959e-01,\n",
       "         -2.77562439e-03,  6.56960234e-02]], dtype=float32)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OnnxRuntime output\n",
    "ort_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.27594674,  0.21984822,  0.25413558, -0.08162472,  0.01668325,\n",
       "          0.25038725,  0.16266353,  0.24064493, -0.10954297, -0.3933645 ,\n",
       "         -0.08273189, -0.21643178,  0.6624239 , -0.06160849, -0.38500285,\n",
       "         -0.10603996,  0.0693749 ,  0.05612227,  0.33561486, -0.22150147,\n",
       "          0.2502498 , -0.09646852,  0.01226454, -0.06253117, -0.20306084,\n",
       "          0.13001284,  0.21155132,  0.48482364, -0.10873769, -0.24222723,\n",
       "          0.0613767 ,  0.15596685,  0.01754492,  0.19280124,  0.26034456,\n",
       "          0.23908499,  0.01173354, -0.06080984, -0.14651725, -0.20623967,\n",
       "         -0.09836124,  0.05493683, -0.06952707, -0.2938555 ,  0.2956541 ,\n",
       "          0.46449935, -0.10821079, -0.24217278, -0.21009654, -0.11673632,\n",
       "         -0.18481727, -0.16723992,  0.00483879, -0.19379617, -0.08032098,\n",
       "          0.2771835 ,  0.2413331 ,  0.06920868,  0.16573788, -0.25973213,\n",
       "          0.19156355, -0.03467644,  0.2368511 ,  0.6559181 ,  0.66242325]],\n",
       "       dtype=float32),\n",
       " array([[ 1.63020894e-01, -2.57163197e-02,  2.59656280e-01,\n",
       "          4.01406050e-01,  2.74486244e-02,  2.98260540e-01,\n",
       "          2.01149955e-01,  1.46165669e-01,  7.52854049e-02,\n",
       "          1.16999894e-01,  1.77018180e-01,  4.82967764e-01,\n",
       "          1.24772936e-01,  1.67430758e-01, -7.95363188e-02,\n",
       "          3.75430286e-01,  6.94021583e-06,  1.20381914e-01,\n",
       "         -2.04938725e-02, -4.68738526e-02,  3.37533891e-01,\n",
       "         -2.00187013e-01, -4.92678657e-02,  4.22583595e-02,\n",
       "          2.16307238e-01,  1.21557146e-01, -2.10626632e-01,\n",
       "         -1.39703423e-01, -3.29561442e-01,  1.34205684e-01,\n",
       "          2.85185277e-02, -3.26583609e-02,  3.52557868e-01,\n",
       "          3.43410820e-02,  2.26066127e-01,  1.60656899e-01,\n",
       "          2.78563648e-01,  1.98015019e-01, -4.56058651e-01,\n",
       "         -6.98197484e-02,  3.87853652e-01,  1.09281391e-03,\n",
       "          1.37577653e-02, -3.61822248e-02, -2.53462017e-01,\n",
       "         -2.38223180e-01, -4.16534126e-01,  7.80062005e-02,\n",
       "          1.79880649e-01,  2.55289376e-02, -1.70910865e-01,\n",
       "         -1.19858682e-01,  6.60568029e-02,  1.08686984e-02,\n",
       "         -3.33188325e-01,  2.24351555e-01, -1.24365553e-01,\n",
       "         -1.69521183e-01,  4.07802641e-01, -5.60722873e-02,\n",
       "          7.33210593e-02,  1.77458435e-01,  3.39106202e-01,\n",
       "          1.24202237e-01,  1.24771863e-01]], dtype=float32)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OnnxRuntime output using actual sequence length\n",
    "opt_outputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch14_gpu",
   "language": "python",
   "name": "torch14_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
